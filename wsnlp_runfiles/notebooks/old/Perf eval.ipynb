{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf13b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py:981: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self.encoder != None, \"the encoder cannot be None\")\n",
      "/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py:984: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self.encoder.config.num_hidden_layers >= max_encoder_num, \"the max encoder number should not exceed defined hidden layer\")\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02f51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "dirname = os.getcwd()\n",
    "\n",
    "source_folder = os.path.join(dirname,'../data/imdb')\n",
    "destination_folder =os.path.join(dirname,'./saved_models/progressive_shrinking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cb73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tokenizer_pretrained='bert-base-uncased', test_only = False):\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_pretrained)\n",
    "\n",
    "    # Model parameter\n",
    "    MAX_SEQ_LEN = 128\n",
    "    PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "    # Fields\n",
    "\n",
    "    label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "    text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                       fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "    # fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
    "    fields = [('text', text_field),('sentiment', label_field)]\n",
    "\n",
    "    # TabularDataset\n",
    "\n",
    "    train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
    "                                               test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "    # Iterators\n",
    "    if test_only:\n",
    "        return Iterator(test, batch_size=16, device=device, train=False, shuffle=True, sort=False)\n",
    "\n",
    "    train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                                device=device, train=True, sort=True, sort_within_batch=True)\n",
    "    valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                                device=device, train=True, sort=True, sort_within_batch=True)\n",
    "    test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=True, sort=False)\n",
    "    return train_iter, valid_iter,test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27042b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self,options_name=\"bert-base-uncased\"):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d002cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERT_for_inference(nn.Module):\n",
    "\n",
    "    def __init__(self,options_name=\"bert-base-uncased\"):\n",
    "        super(BERT_for_inference, self).__init__()\n",
    "\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text):\n",
    "        self.encoder(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260633c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_mock(model, test_loader ):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (text,sentiment), _ in test_loader:\n",
    "            text = text.type(torch.LongTensor)           \n",
    "            text = text.to(device)\n",
    "            sentiment = sentiment.type(torch.LongTensor)  \n",
    "            sentiment = sentiment.to(device)\n",
    "            \n",
    "            ## model inference starts\n",
    "#             output = model(text, sentiment)\n",
    "            return count_ops(model,text)\n",
    "            ## model inference ends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef54099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-12_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-12_H-128_A-2 and are newly initialized: ['bert.encoder.layer.9.attention.output.desnse.linear.weight', 'bert.encoder.layer.0.attention.self.query.linear.weight', 'bert.encoder.layer.4.attention.self.value.linear.bias', 'bert.encoder.layer.10.attention.self.query.linear.weight', 'bert.encoder.layer.2.attention.self.key.linear.weight', 'bert.encoder.layer.3.attention.output.desnse.linear.weight', 'bert.encoder.layer.3.attention.self.key.linear.bias', 'bert.encoder.layer.4.attention.self.value.linear.weight', 'bert.encoder.layer.11.attention.self.value.linear.weight', 'bert.encoder.layer.11.attention.self.key.linear.bias', 'bert.encoder.layer.5.attention.self.value.linear.bias', 'bert.encoder.layer.2.attention.self.value.linear.weight', 'bert.encoder.layer.9.attention.self.key.linear.weight', 'bert.encoder.layer.6.attention.self.key.linear.bias', 'bert.encoder.layer.6.attention.self.query.linear.weight', 'classifier.bias', 'bert.encoder.layer.5.attention.output.desnse.linear.bias', 'bert.encoder.layer.5.attention.self.key.linear.bias', 'bert.encoder.layer.7.attention.self.key.linear.weight', 'bert.encoder.layer.4.attention.output.desnse.linear.bias', 'bert.encoder.layer.11.attention.self.query.linear.weight', 'bert.encoder.layer.4.attention.self.key.linear.bias', 'bert.encoder.layer.8.attention.self.value.linear.bias', 'bert.encoder.layer.7.attention.self.value.linear.weight', 'bert.encoder.layer.9.attention.self.value.linear.weight', 'bert.encoder.layer.1.attention.self.value.linear.weight', 'bert.encoder.layer.4.attention.self.query.linear.bias', 'bert.encoder.layer.1.attention.self.value.linear.bias', 'bert.encoder.layer.9.attention.self.query.linear.weight', 'bert.encoder.layer.3.attention.output.desnse.linear.bias', 'bert.encoder.layer.0.attention.self.key.linear.weight', 'bert.encoder.layer.7.attention.output.desnse.linear.weight', 'bert.encoder.layer.4.attention.output.desnse.linear.weight', 'bert.encoder.layer.4.attention.self.key.linear.weight', 'bert.encoder.layer.1.attention.output.desnse.linear.bias', 'bert.encoder.layer.8.attention.self.key.linear.bias', 'bert.encoder.layer.9.attention.output.desnse.linear.bias', 'bert.encoder.layer.2.attention.self.value.linear.bias', 'bert.encoder.layer.0.attention.output.desnse.linear.bias', 'bert.encoder.layer.8.attention.self.query.linear.weight', 'bert.encoder.layer.10.attention.self.query.linear.bias', 'bert.encoder.layer.1.attention.self.query.linear.weight', 'classifier.weight', 'bert.encoder.layer.3.attention.self.query.linear.bias', 'bert.encoder.layer.0.attention.self.query.linear.bias', 'bert.encoder.layer.7.attention.self.key.linear.bias', 'bert.encoder.layer.0.attention.output.desnse.linear.weight', 'bert.encoder.layer.7.attention.self.query.linear.bias', 'bert.encoder.layer.5.attention.self.value.linear.weight', 'bert.encoder.layer.11.attention.self.key.linear.weight', 'bert.encoder.layer.6.attention.output.desnse.linear.weight', 'bert.encoder.layer.6.attention.self.value.linear.weight', 'bert.encoder.layer.1.attention.self.key.linear.weight', 'bert.encoder.layer.2.attention.self.query.linear.bias', 'bert.encoder.layer.1.attention.self.query.linear.bias', 'bert.encoder.layer.10.attention.output.desnse.linear.weight', 'bert.encoder.layer.8.attention.self.value.linear.weight', 'bert.encoder.layer.10.attention.self.value.linear.weight', 'bert.encoder.layer.11.attention.output.desnse.linear.bias', 'bert.encoder.layer.3.attention.self.value.linear.bias', 'bert.encoder.layer.8.attention.self.query.linear.bias', 'bert.encoder.layer.5.attention.self.key.linear.weight', 'bert.encoder.layer.11.attention.self.query.linear.bias', 'bert.encoder.layer.6.attention.self.key.linear.weight', 'bert.encoder.layer.3.attention.self.query.linear.weight', 'bert.encoder.layer.3.attention.self.key.linear.weight', 'bert.encoder.layer.5.attention.self.query.linear.bias', 'bert.encoder.layer.11.attention.self.value.linear.bias', 'bert.encoder.layer.2.attention.output.desnse.linear.weight', 'bert.encoder.layer.3.attention.self.value.linear.weight', 'bert.encoder.layer.10.attention.self.value.linear.bias', 'bert.encoder.layer.10.attention.self.key.linear.bias', 'bert.encoder.layer.2.attention.output.desnse.linear.bias', 'bert.encoder.layer.4.attention.self.query.linear.weight', 'bert.encoder.layer.7.attention.output.desnse.linear.bias', 'bert.encoder.layer.5.attention.output.desnse.linear.weight', 'bert.encoder.layer.0.attention.self.key.linear.bias', 'bert.encoder.layer.6.attention.self.query.linear.bias', 'bert.encoder.layer.6.attention.output.desnse.linear.bias', 'bert.encoder.layer.8.attention.self.key.linear.weight', 'bert.encoder.layer.1.attention.output.desnse.linear.weight', 'bert.encoder.layer.5.attention.self.query.linear.weight', 'bert.encoder.layer.10.attention.output.desnse.linear.bias', 'bert.encoder.layer.7.attention.self.query.linear.weight', 'bert.encoder.layer.11.attention.output.desnse.linear.weight', 'bert.encoder.layer.9.attention.self.value.linear.bias', 'bert.encoder.layer.0.attention.self.value.linear.weight', 'bert.encoder.layer.7.attention.self.value.linear.bias', 'bert.encoder.layer.9.attention.self.key.linear.bias', 'bert.encoder.layer.1.attention.self.key.linear.bias', 'bert.encoder.layer.2.attention.self.key.linear.bias', 'bert.encoder.layer.9.attention.self.query.linear.bias', 'bert.encoder.layer.2.attention.self.query.linear.weight', 'bert.encoder.layer.10.attention.self.key.linear.weight', 'bert.encoder.layer.6.attention.self.value.linear.bias', 'bert.encoder.layer.8.attention.output.desnse.linear.weight', 'bert.encoder.layer.0.attention.self.value.linear.bias', 'bert.encoder.layer.8.attention.output.desnse.linear.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BERT('google/bert_uncased_L-12_H-128_A-2').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd310568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"google/bert_uncased_L-12_H-128_A-2\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014099e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (encoder): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (key): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (value): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (desnse): DynamicLinear(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.set_attention_ratio(0.5)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b63a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = create_dataset('google/bert_uncased_L-12_H-128_A-2', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da57749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_attention_head_size :  32\n",
      "active_all_head_size :  64\n",
      "query_layer shape :  tensor([[[[-0.0299, -0.0902, -0.2592,  ...,  0.0430, -0.1856,  0.1938],\n",
      "          [ 0.2604,  0.4037, -0.1294,  ...,  0.4744, -0.1280,  0.0623],\n",
      "          [-0.1937,  0.4894, -0.0918,  ..., -0.0568, -0.4527, -0.0615],\n",
      "          ...,\n",
      "          [ 0.0393,  0.0085,  0.0103,  ...,  0.3954, -0.0845,  0.2589],\n",
      "          [ 0.1186,  0.2164, -0.0742,  ...,  0.3781, -0.1545,  0.1301],\n",
      "          [ 0.2374,  0.3051, -0.0989,  ...,  0.2489, -0.1785,  0.0138]],\n",
      "\n",
      "         [[-0.1585,  0.1640,  0.3701,  ..., -0.0520, -0.1855,  0.2997],\n",
      "          [-0.5947, -0.0562,  0.2200,  ..., -0.1106,  0.1878,  0.2650],\n",
      "          [-0.1355,  0.0229,  0.1192,  ..., -0.1424,  0.1240, -0.0241],\n",
      "          ...,\n",
      "          [-0.2013, -0.3482,  0.0724,  ..., -0.3078, -0.1164,  0.3388],\n",
      "          [-0.1452, -0.2435, -0.0491,  ..., -0.4183, -0.0808,  0.3846],\n",
      "          [-0.0690, -0.1144, -0.1393,  ..., -0.3868, -0.0632,  0.3792]]],\n",
      "\n",
      "\n",
      "        [[[-0.0299, -0.0902, -0.2592,  ...,  0.0430, -0.1856,  0.1938],\n",
      "          [-0.2024, -0.0851,  0.2022,  ..., -0.3212, -0.4310, -0.0557],\n",
      "          [-0.0855,  0.2861, -0.0865,  ...,  0.1357, -0.6133, -0.3033],\n",
      "          ...,\n",
      "          [ 0.0985, -0.0461, -0.0058,  ...,  0.4392, -0.0779,  0.3112],\n",
      "          [ 0.0929,  0.0994, -0.2985,  ...,  0.3552, -0.2443,  0.0553],\n",
      "          [ 0.2439, -0.1233, -0.1727,  ..., -0.2715, -0.1592,  0.0949]],\n",
      "\n",
      "         [[-0.1585,  0.1640,  0.3701,  ..., -0.0520, -0.1855,  0.2997],\n",
      "          [-0.5427, -0.1866,  0.3743,  ...,  0.0110,  0.0394, -0.1089],\n",
      "          [-0.1089, -0.1845,  0.0527,  ..., -0.3178,  0.1740, -0.2142],\n",
      "          ...,\n",
      "          [ 0.2242, -0.4392,  0.2412,  ..., -0.3336, -0.2437, -0.3517],\n",
      "          [ 0.0403,  0.1738, -0.0231,  ..., -0.2605,  0.3138,  0.0497],\n",
      "          [ 0.2856, -0.1377,  0.1311,  ...,  0.0973,  0.2455, -0.3494]]],\n",
      "\n",
      "\n",
      "        [[[-0.0299, -0.0902, -0.2592,  ...,  0.0430, -0.1856,  0.1938],\n",
      "          [-0.1688, -0.0704,  0.1449,  ..., -0.3413, -0.0622,  0.0069],\n",
      "          [ 0.1260,  0.3751,  0.1545,  ..., -0.0257,  0.0701,  0.0482],\n",
      "          ...,\n",
      "          [ 0.1449,  0.0676, -0.1680,  ...,  0.1406, -0.1321, -0.1036],\n",
      "          [ 0.1795, -0.1384,  0.2507,  ..., -0.2026, -0.0441, -0.1470],\n",
      "          [ 0.3852,  0.2202, -0.1270,  ..., -0.1608,  0.1845, -0.1803]],\n",
      "\n",
      "         [[-0.1585,  0.1640,  0.3701,  ..., -0.0520, -0.1855,  0.2997],\n",
      "          [-0.3871, -0.0603,  0.2858,  ..., -0.0020, -0.0174,  0.1432],\n",
      "          [-0.3258, -0.0561, -0.1366,  ...,  0.4272, -0.3495,  0.1181],\n",
      "          ...,\n",
      "          [ 0.0758, -0.5110,  0.2508,  ..., -0.0906, -0.2957,  0.1294],\n",
      "          [ 0.6247, -0.2650,  0.0012,  ..., -0.4858, -0.2377,  0.2166],\n",
      "          [ 0.0096, -0.1397, -0.2049,  ..., -0.0749, -0.2936,  0.1560]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0299, -0.0902, -0.2592,  ...,  0.0430, -0.1856,  0.1938],\n",
      "          [-0.1815,  0.2426,  0.1052,  ...,  0.0184, -0.1947,  0.1147],\n",
      "          [-0.0993,  0.2990, -0.1351,  ...,  0.1372, -0.2009, -0.2068],\n",
      "          ...,\n",
      "          [-0.4098,  0.3722, -0.2696,  ...,  0.3637,  0.0318,  0.0609],\n",
      "          [ 0.1687,  0.1253,  0.0075,  ..., -0.1776, -0.1895, -0.1327],\n",
      "          [ 0.5012,  0.4259, -0.3076,  ...,  0.2643, -0.1389, -0.0557]],\n",
      "\n",
      "         [[-0.1585,  0.1640,  0.3701,  ..., -0.0520, -0.1855,  0.2997],\n",
      "          [-0.3281, -0.1893,  0.0777,  ...,  0.0053, -0.1920, -0.1033],\n",
      "          [-0.2663, -0.3768,  0.2391,  ...,  0.0149, -0.1461,  0.1303],\n",
      "          ...,\n",
      "          [ 0.0694, -0.5005,  0.1155,  ..., -0.0437, -0.3511,  0.1066],\n",
      "          [ 0.4235, -0.3523, -0.0520,  ..., -0.3957, -0.0879, -0.1488],\n",
      "          [-0.1517, -0.1204,  0.0260,  ..., -0.1788,  0.0459,  0.2092]]],\n",
      "\n",
      "\n",
      "        [[[-0.0299, -0.0902, -0.2592,  ...,  0.0430, -0.1856,  0.1938],\n",
      "          [-0.3612, -0.2781, -0.0931,  ...,  0.1910, -0.3785,  0.0211],\n",
      "          [-0.1384,  0.3641,  0.1261,  ..., -0.0925,  0.0604,  0.2380],\n",
      "          ...,\n",
      "          [ 0.2796, -0.1901, -0.3351,  ...,  0.1538, -0.0626,  0.1595],\n",
      "          [-0.0653, -0.3569, -0.4289,  ..., -0.0129,  0.0571,  0.1211],\n",
      "          [ 0.0668, -0.0178, -0.4886,  ...,  0.0155,  0.0854, -0.1663]],\n",
      "\n",
      "         [[-0.1585,  0.1640,  0.3701,  ..., -0.0520, -0.1855,  0.2997],\n",
      "          [-0.2909, -0.2224,  0.0329,  ..., -0.1175,  0.0383, -0.0705],\n",
      "          [-0.3519, -0.2866,  0.0703,  ...,  0.0557, -0.4299,  0.1127],\n",
      "          ...,\n",
      "          [ 0.1145,  0.0080,  0.0548,  ...,  0.2260,  0.1971,  0.1219],\n",
      "          [-0.2405, -0.3108, -0.2118,  ..., -0.1497, -0.2246, -0.0777],\n",
      "          [-0.1681, -0.1520, -0.3273,  ...,  0.0913,  0.0558,  0.1577]]],\n",
      "\n",
      "\n",
      "        [[[-0.0299, -0.0902, -0.2592,  ...,  0.0430, -0.1856,  0.1938],\n",
      "          [-0.0202,  0.2359, -0.0238,  ...,  0.1947, -0.1683, -0.1704],\n",
      "          [-0.0502,  0.4174, -0.1690,  ...,  0.2219,  0.0470, -0.1265],\n",
      "          ...,\n",
      "          [ 0.3041,  0.0573, -0.0521,  ...,  0.4294, -0.0268,  0.1135],\n",
      "          [ 0.0148, -0.1042, -0.2784,  ..., -0.1559,  0.0054, -0.0527],\n",
      "          [ 0.3852,  0.2202, -0.1270,  ..., -0.1608,  0.1845, -0.1803]],\n",
      "\n",
      "         [[-0.1585,  0.1640,  0.3701,  ..., -0.0520, -0.1855,  0.2997],\n",
      "          [-0.2867, -0.1969,  0.2751,  ..., -0.0345, -0.0823,  0.1704],\n",
      "          [-0.3368, -0.1010,  0.2335,  ..., -0.1620, -0.0087,  0.3291],\n",
      "          ...,\n",
      "          [ 0.6134, -0.0068,  0.0704,  ..., -0.1333, -0.2125,  0.2362],\n",
      "          [ 0.1123,  0.0375, -0.2694,  ..., -0.2380, -0.0783, -0.2122],\n",
      "          [ 0.0096, -0.1397, -0.2049,  ..., -0.0749, -0.2936,  0.1560]]]],\n",
      "       device='cuda:6')\n",
      "torch.Size([16, 2, 128, 64])\n",
      "after resizing : \n",
      "torch.Size([16, 2, 128, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 128, 64]' is invalid for input of size 262144",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15023/2082545139.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#         if step < warmup:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             model(text, sentiment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15023/3950777271.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, label)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1657\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         )\n\u001b[0;32m-> 1116\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     ):\n\u001b[0;32m--> 496\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_ofa/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilee300/workspace/nlp_ofa/transformers/src/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_all_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 128, 64]' is invalid for input of size 262144"
     ]
    }
   ],
   "source": [
    "\n",
    "# total_step = 0\n",
    "# total_time = 0\n",
    "# entry_size = 0\n",
    "# warmup = 100\n",
    "# total_step = 1000\n",
    "with torch.no_grad():\n",
    "    for step,((text,sentiment), _) in enumerate(test_iter):\n",
    "\n",
    "        text = text.type(torch.LongTensor)           \n",
    "        text = text.to(device)\n",
    "        sentiment = sentiment.type(torch.LongTensor)  \n",
    "        sentiment = sentiment.to(device)\n",
    "        model(text,sentiment)\n",
    "#         if step < warmup:\n",
    "#             model(text, sentiment)\n",
    "#             entry_size = len(text)\n",
    "#         elif step < warmup + total_step:\n",
    "#             start = time.time()\n",
    "\n",
    "#             out = model(text,sentiment)\n",
    "\n",
    "#             elapsed = time.time() - start\n",
    "#             print(elapsed)\n",
    "#             total_time += elapsed\n",
    "#         else:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be6e766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the test_iter\n",
      "loading the pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-12_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-12_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"creating the test_iter\")\n",
    "test_iter = create_dataset('google/bert_uncased_L-12_H-128_A-2', True)\n",
    "print(\"loading the pretrained model\")\n",
    "model = BERT('google/bert_uncased_L-12_H-128_A-2').to(device)\n",
    "model.eval()\n",
    "with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/test'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    ") as prof:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step,((text,sentiment), _) in enumerate(test_iter):\n",
    "            if step >= (1 + 1 + 3) * 2:\n",
    "                break\n",
    "            text = text.type(torch.LongTensor)           \n",
    "            text = text.to(device)\n",
    "            sentiment = sentiment.type(torch.LongTensor)  \n",
    "            sentiment = sentiment.to(device)\n",
    "            model(text, sentiment)\n",
    "\n",
    "            prof.step()  # Need to call this at the end of each step to notify profiler of steps' boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c733602",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/test'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    ") as prof:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step,((text,sentiment), _) in enumerate(test_iter):\n",
    "            if step >= (1 + 1 + 3) * 2:\n",
    "                break\n",
    "            text = text.type(torch.LongTensor)           \n",
    "            text = text.to(device)\n",
    "            sentiment = sentiment.type(torch.LongTensor)  \n",
    "            sentiment = sentiment.to(device)\n",
    "            model(text, sentiment)\n",
    "\n",
    "            prof.step()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d153c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "varying_encoder_layer_model_list=['google/bert_uncased_L-2_H-768_A-12','google/bert_uncased_L-4_H-768_A-12','google/bert_uncased_L-6_H-768_A-12,''google/bert_uncased_L-8_H-768_A-12','google/bert_uncased_L-10_H-768_A-12','google/bert_uncased_L-12_H-768_A-12']\n",
    "varying_hidden_dim_model_list = ['google/bert_uncased_L-12_H-128_A-2','google/bert_uncased_L-12_H-256_A-4','google/bert_uncased_L-12_H-512_A-8','google/bert_uncased_L-12_H-768_A-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5feca0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_varying_encoder =[]\n",
    "flops_varying_hd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9288e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pretrained in varying_encoder_layer_model_list:\n",
    "    print(\"testing model \", pretrained)\n",
    "    print(\"creating the test_iter\")\n",
    "    test_iter = create_dataset(pretrained, True)\n",
    "    print(\"loading the pretrained model\")\n",
    "    model = BERT(pretrained).to(device)\n",
    "    print(\"beginning inference mock\")\n",
    "    with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=4, repeat=5),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/'+pretrained),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step,((text,sentiment), _) in enumerate(test_iter):\n",
    "                if step >= (1 + 1 + 3) * 5:\n",
    "                    break\n",
    "                text = text.type(torch.LongTensor)           \n",
    "                text = text.to(device)\n",
    "                sentiment = sentiment.type(torch.LongTensor)  \n",
    "                sentiment = sentiment.to(device)\n",
    "                model(text, sentiment)\n",
    "\n",
    "                prof.step()  \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ecd1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation  OPS    \n",
      "---------  -----  \n",
      "_0         1470   \n",
      "_1         490    \n",
      "_2_conv1   1470   \n",
      "--------   ----   \n",
      "Input size: (1, 5, 7, 7)\n",
      "3,430 FLOPs or approx. 0.00 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3430, [['_0', 1470], ['_1', 490], ['_2_conv1', 1470]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pthflops import count_ops\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(5, 5, 1, 1, 0)\n",
    "        # ... other layers present inside will also be ignored\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)\n",
    "\n",
    "# Create a network and a corresponding input\n",
    "inp = torch.rand(1,5,7,7)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(5, 5, 1, 1, 0),\n",
    "    nn.ReLU(inplace=True),\n",
    "    CustomLayer()\n",
    ")\n",
    "\n",
    "# Count the number of FLOPs\n",
    "count_ops(net, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d01b0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea18d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the test_iter\n",
      "loading the pretrained model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b11e5368df845fb95f928f5de0a5e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-12_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-12_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"creating the test_iter\")\n",
    "test_iter = create_dataset('google/bert_uncased_L-12_H-128_A-2', True)\n",
    "print(\"loading the pretrained model\")\n",
    "model = BERT('google/bert_uncased_L-12_H-128_A-2').to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (text,sentiment), _ in test_iter:\n",
    "        text = text.type(torch.LongTensor)           \n",
    "        text = text.to(device)\n",
    "        sentiment = sentiment.type(torch.LongTensor)  \n",
    "        sentiment = sentiment.to(device)\n",
    "\n",
    "        ## model inference starts\n",
    "        with profile(activities=[ProfilerActivity.CUDA],with_flops=True, profile_memory=True, record_shapes=True) as prof:\n",
    "            model(text, sentiment)\n",
    "        ## model inference ends.\n",
    "#         \n",
    "\n",
    "\n",
    "        table_cuda_self_memory = prof.key_averages()\n",
    "#     .table(sort_by=\"self_cuda_memory_usage\", row_limit=10,max_src_column_width=75)\n",
    "\n",
    "        table_cuda_memory=prof.key_averages()\n",
    "#     .table(sort_by=\"cuda_memory_usage\", row_limit=10,max_src_column_width=75)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "468ab94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[memory] 0.0\n",
      "cudaLaunchKernel 41424.098360655735\n",
      "cudaMalloc 225.44444444444446\n",
      "cudaFree 0.3333333333333333\n",
      "cudaDeviceGetAttribute 0.0\n",
      "cudaMemcpy 34.0\n",
      "cudaFuncSetAttribute 0.8165680473372781\n",
      "cudaEventCreateWithFlags 0.25\n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags 0.8783783783783784\n",
      "cudaEventQuery 2.0\n",
      "cudaEventRecord 0.75\n",
      "cudaDeviceSynchronize 21357279.0\n"
     ]
    }
   ],
   "source": [
    "for entry in table_cuda_self_memory:\n",
    "    print(entry.key, entry.cpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33bc3130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FunctionEventAvg key=cudaFree self_cpu_time=1.000us cpu_time=0.333us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_cuda_self_memory[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a71470e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eeb9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_table = prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10,max_src_column_width=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0d35b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FunctionEventAvg key=[memory] self_cpu_time=0.000us cpu_time=0.000us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaLaunchKernel self_cpu_time=15.161s cpu_time=41.424ms  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaMalloc self_cpu_time=2.029ms cpu_time=225.444us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaFree self_cpu_time=1.000us cpu_time=0.333us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaDeviceGetAttribute self_cpu_time=0.000us cpu_time=0.000us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaMemcpy self_cpu_time=34.000us cpu_time=34.000us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaFuncSetAttribute self_cpu_time=138.000us cpu_time=0.817us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaEventCreateWithFlags self_cpu_time=4.000us cpu_time=0.250us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags self_cpu_time=65.000us cpu_time=0.878us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaEventQuery self_cpu_time=24.000us cpu_time=2.000us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaEventRecord self_cpu_time=9.000us cpu_time=0.750us  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>,\n",
       " <FunctionEventAvg key=cudaDeviceSynchronize self_cpu_time=21.357s cpu_time=21.357s  self_cuda_time=0.000us cuda_time=0.000us input_shapes= cpu_memory_usage=0 cuda_memory_usage=0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_cuda_self_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "958b8ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \\n                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us           0 b           0 b           638  \\n                                       cudaLaunchKernel        41.51%       15.161s        41.51%       15.161s      41.424ms           0 b           0 b           366  \\n                                             cudaMalloc         0.01%       2.029ms         0.01%       2.029ms     225.444us           0 b           0 b             9  \\n                                               cudaFree         0.00%       1.000us         0.00%       1.000us       0.333us           0 b           0 b             3  \\n                                 cudaDeviceGetAttribute         0.00%       0.000us         0.00%       0.000us       0.000us           0 b           0 b            11  \\n                                             cudaMemcpy         0.00%      34.000us         0.00%      34.000us      34.000us           0 b           0 b             1  \\n                                   cudaFuncSetAttribute         0.00%     138.000us         0.00%     138.000us       0.817us           0 b           0 b           169  \\n                               cudaEventCreateWithFlags         0.00%       4.000us         0.00%       4.000us       0.250us           0 b           0 b            16  \\ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%      65.000us         0.00%      65.000us       0.878us           0 b           0 b            74  \\n                                         cudaEventQuery         0.00%      24.000us         0.00%      24.000us       2.000us           0 b           0 b            12  \\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \\nSelf CPU time total: 36.521s\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc153e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a84bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"D:/data.txt\", \"w\")\n",
    " \n",
    "#write string to file\n",
    "text_file.write('Python Tutorial by TutorialKart.')\n",
    " \n",
    "#close file\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4182e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"table_cuda_self_memory.txt\",\"w\")as f:\n",
    "    f.write(my_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f490af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a230a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = BERT_for_inference('google/bert_uncased_L-2_H-768_A-12').to(device)\n",
    "print(\"beginning inference mock\")\n",
    "flops= inference_mock(model,test_iter)\n",
    "print(\"flops : \", flops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
